# Transformer_2017

## 초록

- 지배적인 시퀸스 변환 모델들은 복잡한 순환 신경망(RNN)이나 합성곱 신경망(CNN)에 기반하여 인코더와 디코더를 포함하고 있습니다.

- 가장 성능이 좋은 모델들은 인코더와 디코더를 어텐션 메커니즘으로 연결합니다.

- 우리는 Transformer라는 새로운 단순한 네트워크 아키텍처를 제안하는데, 이는 오직 어텐션 메커니즘만을 사용하여 순환(RNN)이나 합성곱(CNN)을 완전히 제거합니다.

  - 이름도 그래서 **Transformer**라고 붙였습니다.

- 두 가지 기계 번역 과제를 실험한 결과, 이 모델은 더 높은 품질을 보였을 뿐 아니라, 벙렬화가 가능하고, 훈련시간도 크게 단축되었습니다.

- WMT 2014 영어→독일어 번역 과제에서 BLEU 점수 28.4를 기록했으며, 기존 최고 결과(앙상블 포함)보다 2 BLEU 이상 개선되었습니다.

- WMT 2014 영어→프랑스어 번역 과제에서는, 단일 모델 기준으로 BLEU 41.8을 달성했는데 이는 당시 단일 모델 기준 최고 성능을 찍었다.
- 게다가 후년 시간도 훨씬 짧아서, 8개의 GPU로 단 3.5일이면 끝났다.

- 또한, Transformer가 다른 과제에도 잘 일반화됨을 보여주기 위해, **영어 구문 파싱(English constituency parsing)**에 적용하여 대규모·소규모 학습 데이터 환경 모두에서 성공적인 결과를 얻었습니다.

## Introduction(서론)
