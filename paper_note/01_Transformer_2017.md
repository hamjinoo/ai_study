# Transformer_2017

## 초록

- 지배적인 시퀸스 변환 모델들은 복잡한 순환 신경망(RNN)이나 합성곱 신경망(CNN)에 기반하여 인코더와 디코더를 포함하고 있습니다.

- 가장 성능이 좋은 모델들은 인코더와 디코더를 어텐션 메커니즘으로 연결합니다.

- 우리는 Transformer라는 새로운 단순한 네트워크 아키텍처를 제안하는데, 이는 오직 어텐션 메커니즘만을 사용하여 순환(RNN)이나 합성곱(CNN)을 완전히 제거합니다.

  - 이름도 그래서 **Transformer**라고 붙였습니다.

- 두 가지 기계 번역 과제를 실험한 결과, 이 모델은 더 높은 품질을 보였을 뿐 아니라, 벙렬화가 가능하고, 훈련시간도 크게 단축되었습니다.

- WMT 2014 영어→독일어 번역 과제에서 BLEU 점수 28.4를 기록했으며, 기존 최고 결과(앙상블 포함)보다 2 BLEU 이상 개선되었습니다.

- WMT 2014 영어→프랑스어 번역 과제에서는, 단일 모델 기준으로 BLEU 41.8을 달성했는데 이는 당시 단일 모델 기준 최고 성능을 찍었다.
- 게다가 후년 시간도 훨씬 짧아서, 8개의 GPU로 단 3.5일이면 끝났다.

- 또한, Transformer가 다른 과제에도 잘 일반화됨을 보여주기 위해, **영어 구문 파싱(English constituency parsing)**에 적용하여 대규모·소규모 학습 데이터 환경 모두에서 성공적인 결과를 얻었습니다.

## 서론

- 순환 신경망(RNN), 특히 LSTM과 GRU은 언어 모델링과 기계 번역 같은 시퀀스 모델링 및 변환 문제에서 최첨단 방법으로 확고히 자리잡아 왔다.

  - ->RNN 계열(LSTM, GRU)은 2010년대 중반까지 **번역이나 문장 생성** 같은 작업에서 최고의 기술로 인정받았다.

- 그 이후로도 순환 언어 모델(RNN)과 인코더·디코더 구조의 한계를 넓히려는 많은 연구들이 이어졌다.

- 순환 모델은 일반적으로 입력 및 출력 시퀀스의 각 기호(토큰) 위치에 따라 계산을 분할한다. (순서대로 처리)

- 이때 각 위치는 연산 시간의 단계와 정렬되며, 모델은 이전 은닉 상태(ht-1)와 현재 입력을 바탕으로 새로운 은닉 상태(ht)를 생성한다.

  - -> RNN은 문장을 읽을 때, 매 단어마다 **"이전 기억 + 현재 단어"를 합쳐서 새로운 기억**을 만든다.
  - Hidden State (h,) : 지금까지 읽은 정보를 요약해 담는 메모리 벡터

- 이러한 본질적인 순차적 특성 때문에, 학습 시 예제 내부에서 병렬화가 불가능하며, 시퀀스 길이가 길어질수록 메모리 제약 때문에 여러 예제를 동시에 처리하는 것도 어려워진다.

> 여기까지가 서론 첫 부분이고 RNN의 한계를 설명하는 구간입니다.

---

- 최근 연구에서는 분해 기법(factorization tricks)과 조건부 계산(conditional computation)을 통해 계산 효율성을 크게 개선했으며, 특히 후자의 경우 모델 성능까지 향상시켰다.

-그러나, 이러한 개선에도 불구하고 순차적 계산이라는 근본적인 제약은 여전히 남아있다.

- 어텐션 메커니즘은 다양한 작업에서 매력적인 시퀀스 모델링과 변환 모델의 필수 요소가 되었으며, 입력 또는 출력 시퀀스 내에서 거리에 상관없이 의존성을 모델링할 수 있게 해준다.

  - 어텐션 덕분에, 문장에서 멀리 떨어진 단어 간 관계도 잘 잡을 수 있게 되었다. (예: "The cat ... sat on the mat"에서 cat <-> mat 관계)

- 그러나, 일부 예외를 제외하면 이런 어텐션 메커니즘은 대부분 순환 신경망과 함께 사용된다. (어텐션을 쓰더라도 여전히 **RNN 위에 얹어 쓰는 방식**)

- 이 연구에서는 순환 구조를 과감히 버리고, 입력과 출력 사이의 전역적 의존성을 전적으로 어텐션 메커니즘에 의존하는 모델 아키텍처, **Transformer**를 제안한다.

  - RNN을 버리고 Attention만으로 Model을 만들 수 있는데 그것이 Transformer이다.

- Transformer는 훨씬 더 많은 병렬화를 가능하게 하며, 단 8개의 P100 GPU에서 12시간 훈련만으로도 번역 품질에서 새로운 최고 수준에 도달할 수 있다.

> RNN은 느리고 병렬화 불가능, CNN도 시도했지만 한계가 있었다. 다만 어텐션은 멀리 있는 단어 관계를 잘 잡았기에 RNN 없이 어텐션만 쓰는 Transformer 제안

- Encoder·Decoder 구조 : 입력(예: 영어 문장)을 인코더로 벡터로 바꾼 뒤, 디코더로 출력(예: 독일어 문장)을 생성하는 구조
- Attention : 입력 단어들 중 중요한 부분에 더 집중하는 메커니즘
- P100 GPU : 당시 최신 NVIDIA GPU.

---
