# 이론

LLM(Large Language Model) : 엄청 큰 언어 모델델

- 단순히 말을 잘하는 게 아니라 단어 사이 관계를 수치로 계산하는 모델
  - Transformer : 문장을 토큰화하여 쪼갠다음 그걸 Transformer 구조를 통해 처리
  - 어텐션 메커니즘 : 각 단어가 어디에 주목되어야 하는지 계산하는 것

---

- Transformer

  1. 먼저, 문장은 단어 단위로 잘게 쪼개진다.
  2. 각 단어는 숫자로 된 벡터로 바뀌며 여기에 '포지셔녈 인코딩'이 더해지는데

  - 포지셔널 인코딩 : 단어의 순서를 알려주는 정보

  3. 어텐션(각 단어가 문장 내 다른 단어들과 얼마나 중요한지 그 관계를 수학적으로 계산 )
  4. 그렇게 정보가 인코딩에 모이고 디코더는 그걸 참고해서 다음 단어를 예측한다.
     |요약 : 순서 정보를 관계 계산을 통해 문장을 이해하고 그걸 바탕으로 말을 만들어준다. // 다만 이미 학습한 지식에 기반하기에 최신 정보나 상황에 맞게 답하기는 어렵다. -> RAG

- 할루시네이션 : 이미 학습한 것만 가지고 있어서 최신 정보나 내부 문서 등 모르기에 괜히 지어낸다.

- RAG : 검색해서 답변을 생성하는 구조

  1. 질문을 받으면 벡터 데이터베이스에서 관련 문서를 찾는다.
  2. 그 다음 그 문서를 바탕으로 LLM이 대답을 만들어낸다.

  - 예 : 사내 문서를 벡터 DB에 넣어두면 그걸 RAG가 찾아서 GPT가 마치 알고 있는 것처럼 설명할 수 있다.

- 벡터 데이터베이스

  1. 회사 보고서의 문장의 의미를 숫자로 바꿔서 저장 (임베딩)
  2. 그럼 사용자의 질문도 임베딩돼서 숫자 벡터로 바뀌고 DB 문서 중 의미가 가장 비슷한 방향에 있는 벡터 즉, 관련성 높은 문서를 찾아서 꺼낸다. (코사인 유사도도)

- 벡터를 만드는 방법 = 임베딩(무언가를 안에다 심는다)

  - AI에서는 문장의 의미를 숫자 안에 심는 과정이다.
  - 예시
    - 애플 실적 발표, 1분기 수익 증가의 의미는 비슷하다.
    - 이런 문장을 AI 임베딩 모델에 넣으면 벡터가 만들어지는데 여기에 문맥, 구조, 의미 같은 정보가 담겨 있다.
    - 그래서 비슷한 문장은 벡터 공간에서도 가까운 위치에 있게 된다.
    - 이렇게 만들어진 벡터가 벡터 DB에 저장돼서 RAG가 의미가 비슷한 문서를 찾아낼 수 있게 해준다.
  - Embedding을 만들어주는 Model은 OpenAI, Cohee, Hugging face 같은 곳에서 제공

-https://www.youtube.com/shorts/tEd0ILltO_E

---

---

---

## RNN → LSTM/GRU → CNN(ConvS2S) → Transformer

### RNN (Recurrent Neural Network, 1980s~2010s)

- Transformer 이전까지 자연어 처리(NLP)의 주력 모델은 RNN이었습니다.
  → 단어 순서가 있는 문장을 차례대로 처리하는 데 강점이 있었죠. (예: 번역, 음성 인식)
- 한계
  - 긴 문장에서 앞뒤 연결을 잘 못함(장기 의존성 문제)
  - 계산이 순차적이라 병렬화가 안 되고 느림

### LSTM (1997) & GRU (2014)

- LSTM (Hochreiter & Schmidhuber, 1997)
  - "메모리 셀 + 게이트 구조"로 장기 의존성 문제 해결
  - 번역/음성에서 RNN보다 훨씬 나은 성능
- GRU (Cho et al., 2014) - LSTM을 단순화한 구조, 계산이 더 가벼움
  | 의의 : 2010년대 초반 딥러닝 기반 번역 시스템(Google Translate 초기 버전)의 핵심심

### CNN 기반 Seq2Seq (ConvS2S, 2017)

- ConvS2S (Gehring et al., 2017) - 합성곱 신경망(CNN)을 번역에 적용. - 장점: RNN보다 병렬 처리 가능 → 속도 빠름. - 단점: 여전히 멀리 떨어진 단어들 관계(long-range dependency)를 잡기 힘듦.
  | 의의: RNN의 한계를 극복하려는 시도의 한 축.
