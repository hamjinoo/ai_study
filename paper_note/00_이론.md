# 이론

## RNN → LSTM/GRU → CNN(ConvS2S) → Transformer

### RNN (Recurrent Neural Network, 1980s~2010s)

- Transformer 이전까지 자연어 처리(NLP)의 주력 모델은 RNN이었습니다.
  → 단어 순서가 있는 문장을 차례대로 처리하는 데 강점이 있었죠. (예: 번역, 음성 인식)
- 한계
  - 긴 문장에서 앞뒤 연결을 잘 못함(장기 의존성 문제)
  - 계산이 순차적이라 병렬화가 안 되고 느림

### LSTM (1997) & GRU (2014)

- LSTM (Hochreiter & Schmidhuber, 1997)
  - "메모리 셀 + 게이트 구조"로 장기 의존성 문제 해결
  - 번역/음성에서 RNN보다 훨씬 나은 성능
- GRU (Cho et al., 2014) - LSTM을 단순화한 구조, 계산이 더 가벼움
  | 의의 : 2010년대 초반 딥러닝 기반 번역 시스템(Google Translate 초기 버전)의 핵심심

### CNN 기반 Seq2Seq (ConvS2S, 2017)

- ConvS2S (Gehring et al., 2017) - 합성곱 신경망(CNN)을 번역에 적용. - 장점: RNN보다 병렬 처리 가능 → 속도 빠름. - 단점: 여전히 멀리 떨어진 단어들 관계(long-range dependency)를 잡기 힘듦.
  | 의의: RNN의 한계를 극복하려는 시도의 한 축.
