# GPT3_2020

## 초록

- 대규모 텍스트 말뭉치로 사전학습(pre-trading) 후 특정 과제에 대해 미세 조정(fine-tuning)하는 방식이 여러 NLP 과제에서 큰 성과를 보인다고 입증해 왔다.

- 하지만 여전히 수천~수만 개의 과제별 데이터셋이 필요했다.

  - 인간은 몇 가지 예시나 간단한 지시만으로 새로운 언어 과제를 수행할 수 있지만, 현재의 NLP 시스템은 여전히 어려움을 겪었다.

- 이 논문에서 우리는 **언어 모델을 크게 확장할수록 과제에 특화되지 않은 상태에서도(Few-shot setting) 성능이 크게 개선**됨을 보여줍니다.
- 구체적으로 **1750억 개의 파라미터**를 가진 자기회귀 언어 모델 GPT-3를 학습시켰으며, 당시 존재했던 비희소 언어 모델 중 최대 크기의 약 10배입니다.
- GPT-3는 어떤 과제에서도 추가 학습(fine-tuning) 없이, 단순히 텍스트 기반 프롬프트와 몇 개의 예시만으로 작업을 수행했다.

> 핵심 요약: GPT-3는 1750억 파라미터를 가진 초대형 언어 모델로, 몇 개의 예시만으로도(few-shot) 다양한 언어 과제를 잘 수행할 수 있음을 입증했다.

NLP
자기회귀 언어 모델
비희소 언어 모델델
