# ScalingLaws_2020

## 초록

- 우리는 언어 모델 성능(교차 엔트로피 손실 기준)에 대한 **경험적 스케일링 법칙**을 연구한다.

  - 언어 모델을 더 크게 만들거나 데이터를 늘릴 때 성능이 어떻게 변하는지, 실제 실험을 통해 법칙을 찾아보자는 이야기
  - **Scaling laws(스케일링 법칙)** : 모델 크기·데이터 크기·컴퓨트가 커질 때 성능이 어떤 규칙적으로 변하는지 나타내는 법칙
  - Cross-entropy loss(교차 엔트로피 손실) : 모델이 정답과 얼마나 차이나는지 수치로 나타내는 척도

- 손실 값은 **모든 크기, 데이터셋 크기, 훈련 연산량**에 따라 **거듭제곱 법칙**으로 변화하며, 이러한 경향은 **7자릿수 이상 규모**에서도 일관되게 나타난다.

  - 모델을 키우거나 데이터·연산량을 늘리면 성능이 규칙적으로 좋아지는데, 이 패턴은 매우 큰 범위(10⁷배 규모)에서도 유지됩니다.
  - Power-law (거듭제곱 법칙): "x가 커지면 y는 일정한 비율로 줄거나 늘어난다"는 수학적 패턴.
  - 손실(loss) : "모델이 정답과 얼마나 차이가 있는지"를 나타내는 지표, 낮을수록 정답과 가까움
    - 손실 값이 줄어든다 = 성능이 좋아진다.

- 네트워크 폭(width)이나 깊이(depth) 같은 다른 구조적 세부 사항은 넓은 범위에서는 거의 영향을 주지 않는다.

  - 모델 층을 더 깊거나 넓게 만드는 것보다, 단순히 크기와 데이터, 연산량을 늘리는 게 훨씬 중요하다는 의미
  - Network width (폭) : 각 층에 들어가는 뉴런 수 (한 줄에 몇 명이 서 있는가)
  - Network depth (깊이) : 층(layer)의 개수 (몇 층짜리 건물이냐)

- 간단한 수식으로 **모델/데이터 크기에 따른 과적합(overfitting)** 정도와 **모델 크기에 따른 학습 속도**를 설명할 수 있다.

  - 데이터가 부족할 때 큰 모델이 과적합하는 정도나, 큰 모델이 학습하는 속도도 일정한 수학적 규칙을 따른다는 뜻이다.
  - Overfitting : 훈련 데이터는 잘 맞지만, 새로운 데이터에는 잘 못 맞추는 현상

- 이러한 관계를 통해, 주어진 **연산 자원(컴퓨트 예산)**을 어떻게 최적으로 분해해야 하는지 알 수 있다.

  - "같은 컴퓨터 성능을 쓴다면, 모델 크기·데이터·훈련 횟수 중 어디에 힘을 줘야 효율적인지"를 계산할 수 있습니다.
  - Compute budget(컴퓨트 예산) : 훈련에 쓸 수 있는 총 연산 자원(시간, GPU 수 등)

- 큰 모델은 작은 모델보다 훨씬 더 **데이터 효율적 (sample-efficient)**이다. 따라서 최적의 계산 효율 훈련은 **아주 큰 모델을 비교적 적은 데이터로 학습시키고 완전히 수렴(convergence)하기 훨씬 전에 멈추는 것**이다.

  - 작은 모델을 끝까지 열심히 훈련시키는 것보다, **큰 모델을 짧게 훈련하는 것이 더 효율적**이라는 결론이다.
  - Sample efficiency : 같은 성능을 얻기 위해 필요한 데이터 양
  - Convergence : 학습이 더 이상 성능 향상을 보이지 않는 상태

- **질문** : 폭·깊이보다 크기 자체가 더 중요하다고 헀는데 폭 깊이가 많아지면 크기가 커지는 거 아닌가?
- **답변**
  - 모델의 크기는 전체 파라미터 수이며 이 수는 **폭 × 깊이 × (다른 상수 요인들)**로 결정
  - 논문이 말하는 실제 성능 향상에서 “폭·깊이보다 크기 자체가 중요하다”는 뜻은 폭이나 깊이의 크가 아닌 전체 파라미터 수에 의해 결정된다는 의미

> 이 초록은 **언어 모델은 성능은 크기·데이터·컴퓨트에 따라 규칙적으로 좋아지고, 작은 모델을 오래 훈련하는 것보다 큰 모델을 짧게 훈련하는 것이 훨씬 효율적이다**는 결론을 말합니다.
